#!/usr/bin/env python3
import sys
import os
import re
from datetime import datetime

def extract_innodb_buffer_pool_size(content):
    """Extract and convert innodb_buffer_pool_size to GB"""
    match = re.search(r'innodb_buffer_pool_size\s+(\d+)', content)
    if match:
        bytes_size = int(match.group(1))
        gb_size = bytes_size / (1024**3)
        return f"{gb_size:.0f}GB"
    return "N/A"

def extract_innodb_flush_log(content):
    """Extract innodb_flush_log_at_trx_commit value"""
    match = re.search(r'innodb_flush_log_at_trx_commit\s+(\d+)', content)
    return match.group(1) if match else "N/A"

def extract_all_performance_data(content):
    """Extract all test results for all thread counts"""
    lines = content.split('\n')
    results = {}
    
    for line in lines:
        if '| oltp_' in line and line.count('|') > 10:
            parts = [p.strip() for p in line.split('|')]
            if len(parts) > 3 and parts[1] and parts[2]:
                scenario = parts[1]
                threads = parts[2]
                if scenario not in results:
                    results[scenario] = {}
                results[scenario][threads] = {
                    'qps': parts[3] if len(parts) > 3 else '',
                    'tps': parts[4] if len(parts) > 4 else '',
                    'avg_latency': parts[5] if len(parts) > 5 else '',
                    'p95_latency': parts[6] if len(parts) > 6 else '',
                    'cpu_sirq': parts[7] if len(parts) > 7 else '',
                    'cpu_user': parts[8] if len(parts) > 8 else '',
                    'cpu_sys': parts[9] if len(parts) > 9 else '',
                    'cpu_wait': parts[10] if len(parts) > 10 else '',
                    'io_util': parts[11] if len(parts) > 11 else ''
                }
    
    return results

def extract_cpu_memory_info(content):
    """Extract CPU model, cores, and memory info"""
    cpu_match = re.search(r'å‹å·åç§°ï¼š\s*(.+)', content)
    if not cpu_match:
        cpu_match = re.search(r'Model name:\s*(.+)', content)
    cpu_model = cpu_match.group(1).strip() if cpu_match else "N/A"
    
    # Support both "CPU(s):" and "CPU:" formats
    cores_match = re.search(r'CPU\(s\):\s*(\d+)', content)
    if not cores_match:
        cores_match = re.search(r'^CPU:\s+(\d+)', content, re.MULTILINE)
    cores = cores_match.group(1) if cores_match else "N/A"
    
    mem_match = re.search(r'Mem:\s+(\d+\w+)', content)
    memory = mem_match.group(1) if mem_match else "N/A"
    
    return cpu_model, cores, memory

def merge_reports(env_names):
    """Merge multiple performance reports with enhanced details"""
    
    # Read all reports
    reports = {}
    for env in env_names:
        file_path = f"{env}/performance_report.md"
        if os.path.exists(file_path):
            with open(file_path, 'r', encoding='utf-8') as f:
                reports[env] = f.read()
        else:
            print(f"Warning: {file_path} not found")
            return
    
    # Extract data for summary
    env_data = {}
    
    for env, content in reports.items():
        cpu_model, cores, memory = extract_cpu_memory_info(content)
        buffer_size = extract_innodb_buffer_pool_size(content)
        flush_log = extract_innodb_flush_log(content)
        perf_data = extract_all_performance_data(content)
        
        env_data[env] = {
            'cpu_model': cpu_model,
            'cores': cores,
            'memory': memory,
            'buffer_size': buffer_size,
            'flush_log': flush_log,
            'performance': perf_data
        }
    
    # Generate merged report
    output = f"""# MySQL Sysbench æ€§èƒ½æµ‹è¯•ç»¼åˆæŠ¥å‘Š (è¯¦ç»†ç‰ˆ)

## ğŸ“Š æ‰§è¡Œæ‘˜è¦

æœ¬æŠ¥å‘Šæ±‡æ€»äº† **{len(env_names)}** ä¸ªä¸åŒç¯å¢ƒä¸‹çš„MySQLæ€§èƒ½æµ‹è¯•ç»“æœï¼Œæ¶µç›–IDCè‡ªå»ºæœºæˆ¿ã€åä¸ºäº‘å’Œé˜¿é‡Œäº‘ç­‰å¤šç§éƒ¨ç½²åœºæ™¯ã€‚

### æµ‹è¯•ç¯å¢ƒæ¦‚è§ˆ

| ç¯å¢ƒ | CPUå‹å· | æ ¸æ•° | å†…å­˜ | Buffer Pool | Flush Log |
|------|---------|------|------|-------------|-----------|
"""
    
    for env in env_names:
        if env in env_data:
            data = env_data[env]
            output += f"| **{env}** | {data['cpu_model']} | {data['cores']} | {data['memory']} | {data['buffer_size']} | {data['flush_log']} |\n"
    
    output += """
### æµ‹è¯•é…ç½®

- **æµ‹è¯•å·¥å…·**: sysbench + tsar
- **æµ‹è¯•æ•°æ®é›†**: 16è¡¨ Ã— 1000ä¸‡è¡Œ
- **æµ‹è¯•æ—¶é•¿**: æ¯åœºæ™¯30ç§’
- **æµ‹è¯•åœºæ™¯**: ç‚¹æŸ¥è¯¢ã€åªè¯»ã€è¯»å†™æ··åˆã€åªå†™
- **å¹¶å‘çº§åˆ«**: 1, 8, 16, 32, 64, 128 çº¿ç¨‹

---

## ğŸ† æ€§èƒ½æ’å

### ç‚¹æŸ¥è¯¢æ€§èƒ½å¯¹æ¯” (oltp_point_select)

"""
    
    # Point select comparison table
    output += "| ç¯å¢ƒ | 1çº¿ç¨‹ | 8çº¿ç¨‹ | 16çº¿ç¨‹ | 32çº¿ç¨‹ | 64çº¿ç¨‹ | 128çº¿ç¨‹ |\n"
    output += "|------|-------|-------|--------|--------|--------|----------|\n"
    
    for env in env_names:
        if env in env_data and 'oltp_point_select' in env_data[env]['performance']:
            perf = env_data[env]['performance']['oltp_point_select']
            row = f"| **{env}** |"
            for threads in ['1', '8', '16', '32', '64', '128']:
                qps = perf.get(threads, {}).get('qps', '-')
                row += f" {qps} |"
            output += row + "\n"
    
    output += "\n### åªå†™æ€§èƒ½å¯¹æ¯” (oltp_write_only)\n\n"
    output += "| ç¯å¢ƒ | 1çº¿ç¨‹ | 8çº¿ç¨‹ | 16çº¿ç¨‹ | 32çº¿ç¨‹ | 64çº¿ç¨‹ | 128çº¿ç¨‹ |\n"
    output += "|------|-------|-------|--------|--------|--------|----------|\n"
    
    for env in env_names:
        if env in env_data and 'oltp_write_only' in env_data[env]['performance']:
            perf = env_data[env]['performance']['oltp_write_only']
            row = f"| **{env}** |"
            for threads in ['1', '8', '16', '32', '64', '128']:
                qps = perf.get(threads, {}).get('qps', '-')
                row += f" {qps} |"
            output += row + "\n"
    
    output += "\n### è¯»å†™æ··åˆæ€§èƒ½å¯¹æ¯” (oltp_read_write)\n\n"
    output += "| ç¯å¢ƒ | 1çº¿ç¨‹ | 8çº¿ç¨‹ | 16çº¿ç¨‹ | 32çº¿ç¨‹ | 64çº¿ç¨‹ | 128çº¿ç¨‹ |\n"
    output += "|------|-------|-------|--------|--------|--------|----------|\n"
    
    for env in env_names:
        if env in env_data and 'oltp_read_write' in env_data[env]['performance']:
            perf = env_data[env]['performance']['oltp_read_write']
            row = f"| **{env}** |"
            for threads in ['1', '8', '16', '32', '64', '128']:
                qps = perf.get(threads, {}).get('qps', '-')
                row += f" {qps} |"
            output += row + "\n"
    
    output += "\n### åªè¯»æ€§èƒ½å¯¹æ¯” (oltp_read_only)\n\n"
    output += "| ç¯å¢ƒ | 1çº¿ç¨‹ | 8çº¿ç¨‹ | 16çº¿ç¨‹ | 32çº¿ç¨‹ | 64çº¿ç¨‹ | 128çº¿ç¨‹ |\n"
    output += "|------|-------|-------|--------|--------|--------|----------|\n"
    
    for env in env_names:
        if env in env_data and 'oltp_read_only' in env_data[env]['performance']:
            perf = env_data[env]['performance']['oltp_read_only']
            row = f"| **{env}** |"
            for threads in ['1', '8', '16', '32', '64', '128']:
                qps = perf.get(threads, {}).get('qps', '-')
                row += f" {qps} |"
            output += row + "\n"
    
    output += """
---

## ğŸ“ˆ å»¶è¿Ÿåˆ†æ

### ç‚¹æŸ¥è¯¢å»¶è¿Ÿå¯¹æ¯” (95%åˆ†ä½, ms)

| ç¯å¢ƒ | 1çº¿ç¨‹ | 8çº¿ç¨‹ | 16çº¿ç¨‹ | 32çº¿ç¨‹ | 64çº¿ç¨‹ | 128çº¿ç¨‹ |
|------|-------|-------|--------|--------|--------|----------|
"""
    
    for env in env_names:
        if env in env_data and 'oltp_point_select' in env_data[env]['performance']:
            perf = env_data[env]['performance']['oltp_point_select']
            row = f"| **{env}** |"
            for threads in ['1', '8', '16', '32', '64', '128']:
                latency = perf.get(threads, {}).get('p95_latency', '-')
                row += f" {latency} |"
            output += row + "\n"
    
    output += "\n### è¯»å†™æ··åˆå»¶è¿Ÿå¯¹æ¯” (95%åˆ†ä½, ms)\n\n"
    output += "| ç¯å¢ƒ | 1çº¿ç¨‹ | 8çº¿ç¨‹ | 16çº¿ç¨‹ | 32çº¿ç¨‹ | 64çº¿ç¨‹ | 128çº¿ç¨‹ |\n"
    output += "|------|-------|-------|--------|--------|--------|----------|\n"
    
    for env in env_names:
        if env in env_data and 'oltp_read_write' in env_data[env]['performance']:
            perf = env_data[env]['performance']['oltp_read_write']
            row = f"| **{env}** |"
            for threads in ['1', '8', '16', '32', '64', '128']:
                latency = perf.get(threads, {}).get('p95_latency', '-')
                row += f" {latency} |"
            output += row + "\n"
    
    output += """
---

## ğŸ’¡ å…³é”®å‘ç°

### æ€§èƒ½ç‰¹ç‚¹

1. **CPUæ¶æ„å½±å“**
   - ä¸åŒCPUæ¶æ„åœ¨å„åœºæ™¯ä¸‹è¡¨ç°å·®å¼‚æ˜æ˜¾
   - å•æ ¸æ€§èƒ½å¯¹ç‚¹æŸ¥è¯¢åœºæ™¯å½±å“æ˜¾è‘—

2. **äº‹åŠ¡æŒä¹…åŒ–è®¾ç½®**
   - innodb_flush_log_at_trx_commit=1 vs 2 å¯¹å†™å…¥æ€§èƒ½å½±å“æ˜æ˜¾
   - å»ºè®®æ ¹æ®ä¸šåŠ¡å¯¹æ•°æ®å®‰å…¨æ€§è¦æ±‚é€‰æ‹©åˆé€‚é…ç½®

3. **å¹¶å‘æ‰©å±•æ€§**
   - å„ç¯å¢ƒåœ¨ä¸åŒå¹¶å‘çº§åˆ«ä¸‹çš„æ‰©å±•æ€§è¡¨ç°ä¸åŒ
   - éœ€è¦æ ¹æ®å®é™…ä¸šåŠ¡å¹¶å‘é€‰æ‹©åˆé€‚çš„ç¡¬ä»¶é…ç½®

4. **å»¶è¿Ÿæ§åˆ¶**
   - é«˜å¹¶å‘ä¸‹å»¶è¿Ÿæ§åˆ¶èƒ½åŠ›ä½“ç°ç³»ç»Ÿç¨³å®šæ€§
   - 95%åˆ†ä½å»¶è¿Ÿæ˜¯è¡¡é‡ç”¨æˆ·ä½“éªŒçš„é‡è¦æŒ‡æ ‡

### ç¯å¢ƒæ¨è

"""
    
    # Find best performers
    best_point_select = max(env_names, key=lambda x: int(env_data[x]['performance'].get('oltp_point_select', {}).get('128', {}).get('qps', '0').replace(',', '')) if env_data[x]['performance'].get('oltp_point_select', {}).get('128', {}).get('qps', '0').replace(',', '').isdigit() else 0)
    best_write = max(env_names, key=lambda x: int(env_data[x]['performance'].get('oltp_write_only', {}).get('128', {}).get('qps', '0').replace(',', '')) if env_data[x]['performance'].get('oltp_write_only', {}).get('128', {}).get('qps', '0').replace(',', '').isdigit() else 0)
    
    output += f"- **æŸ¥è¯¢å¯†é›†å‹ä¸šåŠ¡**: æ¨è **{best_point_select}** ç¯å¢ƒ\n"
    output += f"- **å†™å…¥å¯†é›†å‹ä¸šåŠ¡**: æ¨è **{best_write}** ç¯å¢ƒ\n"
    output += "- **æ··åˆè´Ÿè½½**: éœ€è¦ç»¼åˆè€ƒè™‘QPSã€å»¶è¿Ÿå’Œæˆæœ¬\n"
    
    output += """
---

## ğŸ“Š 64çº¿ç¨‹æ€§èƒ½å¯¹æ¯”

| æµ‹è¯•åœºæ™¯ | idc | idc.trx1 | huawei | aliyun | aliyun.trx1 |
|---------|-----|----------|--------|--------|-------------|
"""
    
    # Add 64-thread comparison for all scenarios
    scenarios = ['oltp_point_select', 'oltp_read_only', 'oltp_read_write', 'oltp_write_only']
    scenario_names = {
        'oltp_point_select': 'ç‚¹æŸ¥è¯¢',
        'oltp_read_only': 'åªè¯»',
        'oltp_read_write': 'è¯»å†™æ··åˆ',
        'oltp_write_only': 'åªå†™'
    }
    
    for scenario in scenarios:
        row = f"| **{scenario_names.get(scenario, scenario)}** |"
        for env in env_names:
            if env in env_data and scenario in env_data[env]['performance']:
                qps = env_data[env]['performance'][scenario].get('64', {}).get('qps', '-')
                row += f" {qps} |"
            else:
                row += " - |"
        output += row + "\n"
    
    output += "\n---\n\n"
    
    # Add individual chapters with full details
    for i, env in enumerate(env_names, 1):
        if env in reports:
            chapter_num = ['ä¸€', 'äºŒ', 'ä¸‰', 'å››', 'äº”', 'å…­', 'ä¸ƒ', 'å…«', 'ä¹', 'å'][i-1]
            output += f"# ç¬¬{chapter_num}ç« ï¼š{env} ç¯å¢ƒè¯¦ç»†æŠ¥å‘Š\n\n"
            
            # Extract content after first header
            content = reports[env]
            lines = content.split('\n')
            start_idx = 0
            for j, line in enumerate(lines):
                if line.startswith('**æµ‹è¯•æ—¶é—´**'):
                    start_idx = j
                    break
            
            chapter_content = '\n'.join(lines[start_idx:])
            
            # Process chapter content
            chapter_lines = chapter_content.split('\n')
            processed_lines = []
            in_table = False
            skip_section = False
            
            for line in chapter_lines:
                # Skip duplicate sections and timestamp placeholders
                if (line.startswith('### ç›‘æ§æ•°æ®è¯´æ˜') or 
                    line.startswith('## æµ‹è¯•ç»“æœåˆ†æ') or 
                    line.startswith('### æ€§èƒ½æŒ‡æ ‡') or
                    line.startswith('### ç³»ç»Ÿç›‘æ§æŒ‡æ ‡') or
                    line.startswith('### å…³é”®å‘ç°') or
                    line.startswith('## è¯´æ˜') or
                    '*æŠ¥å‘Šç”Ÿæˆæ—¶é—´' in line):
                    skip_section = True
                    if '*æŠ¥å‘Šç”Ÿæˆæ—¶é—´' in line:
                        continue
                    continue
                elif skip_section and (line.startswith('#') or line.startswith('---')):
                    skip_section = False
                elif skip_section:
                    continue
                
                # Process table
                if '| æµ‹è¯•åœºæ™¯ | å¹¶å‘æ•° | QPS |' in line:
                    in_table = True
                    parts = line.split('|')
                    if 'ç›‘æ§æ ·æœ¬æ•°' in line:
                        new_parts = [p for p in parts if 'ç›‘æ§æ ·æœ¬æ•°' not in p]
                        processed_lines.append('|'.join(new_parts))
                    else:
                        processed_lines.append(line)
                elif in_table and line.startswith('|------'):
                    parts = line.split('|')
                    if len(parts) > 13:
                        new_parts = parts[:12] + parts[13:]
                        processed_lines.append('|'.join(new_parts))
                    else:
                        processed_lines.append(line)
                elif in_table and line.startswith('| oltp_'):
                    parts = line.split('|')
                    if len(parts) > 13:
                        new_parts = parts[:12] + parts[13:]
                        processed_lines.append('|'.join(new_parts))
                    else:
                        processed_lines.append(line)
                elif in_table and (line.strip() == '' or not line.startswith('|')):
                    in_table = False
                    processed_lines.append(line)
                else:
                    processed_lines.append(line)
            
            output += '\n'.join(processed_lines)
            output += "\n\n---\n\n"
    
    # Add appendix
    output += """
# é™„å½•ï¼šç›‘æ§æŒ‡æ ‡è¯´æ˜

## ç›‘æ§æ•°æ®è¯´æ˜

| æŠ¥å‘Šåˆ—å | tsarå¯¹åº”åˆ— | è¯´æ˜ |
|---------|-----------|------|
| CPUè½¯ä¸­æ–­(%) | sirq | è½¯ä¸­æ–­CPUä½¿ç”¨ç‡ |
| CPUç”¨æˆ·(%) | user | ç”¨æˆ·æ€CPUä½¿ç”¨ç‡ |
| CPUç³»ç»Ÿ(%) | sys | å†…æ ¸æ€CPUä½¿ç”¨ç‡ |
| CPUç­‰å¾…(%) | wait | IOç­‰å¾…æ—¶é—´å ç”¨çš„CPU |
| IOåˆ©ç”¨ç‡(%) | util (IOéƒ¨åˆ†) | ç£ç›˜IOä½¿ç”¨ç‡ |

## æ€§èƒ½æŒ‡æ ‡è¯´æ˜

- **QPS (Queries Per Second)**: æ¯ç§’æŸ¥è¯¢æ•°ï¼Œè¡¡é‡æ•°æ®åº“å¤„ç†æŸ¥è¯¢çš„èƒ½åŠ›
- **TPS (Transactions Per Second)**: æ¯ç§’äº‹åŠ¡æ•°ï¼Œä¸QPSåœ¨ç‚¹æŸ¥è¯¢åœºæ™¯ä¸‹ç›¸ç­‰
- **å¹³å‡å»¶è¿Ÿ**: æ‰€æœ‰è¯·æ±‚çš„å¹³å‡å“åº”æ—¶é—´
- **95%å»¶è¿Ÿ**: 95%çš„è¯·æ±‚å“åº”æ—¶é—´ä¸è¶…è¿‡æ­¤å€¼ï¼Œæ›´èƒ½åæ˜ ç”¨æˆ·ä½“éªŒ

## æµ‹è¯•åœºæ™¯è¯´æ˜

| åœºæ™¯ | æè¿° | ä¸»è¦æŒ‡æ ‡ |
|------|------|----------|
| oltp_point_select | ä¸»é”®ç‚¹æŸ¥è¯¢ | QPS, å»¶è¿Ÿ |
| oltp_read_only | åªè¯»äº‹åŠ¡(åŒ…å«å¤šè¡¨JOIN) | TPS, QPS |
| oltp_read_write | è¯»å†™æ··åˆäº‹åŠ¡ | TPS, å»¶è¿Ÿ |
| oltp_write_only | åªå†™äº‹åŠ¡ | TPS, IOåˆ©ç”¨ç‡ |

## æ•°æ®æ¥æºè¯´æ˜

- CPU/IOæ•°æ®æ¥æºäºtsarç›‘æ§æ—¥å¿—ï¼ŒæŒ‰æµ‹è¯•æ—¶é—´æ®µç²¾ç¡®åŒ¹é…å¹¶è®¡ç®—å¹³å‡å€¼
- ç³»ç»Ÿç›‘æ§æ•°æ®ä¸æ€§èƒ½æ•°æ®æ—¶é—´ç²¾ç¡®å¯¹åº”ï¼Œç¡®ä¿æ•°æ®å‡†ç¡®æ€§
- æµ‹è¯•ä½¿ç”¨sysbenchå·¥å…·ï¼Œé’ˆå¯¹MySQLæ•°æ®åº“è¿›è¡Œæ ‡å‡†åŒ–æ€§èƒ½æµ‹è¯•

---

"""
    
    # Add timestamp
    output += f"*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n"
    
    # Write output
    with open('mysql_sysbench_v2.md', 'w', encoding='utf-8') as f:
        f.write(output)
    
    print(f"è¯¦ç»†ç‰ˆåˆå¹¶æŠ¥å‘Šå·²ç”Ÿæˆ: mysql_sysbench_v2.md")

if __name__ == "__main__":
    if len(sys.argv) != 2:
        print("Usage: python3 merge_reports_v2.py env1,env2,env3,...")
        sys.exit(1)
    
    env_names = sys.argv[1].split(',')
    merge_reports(env_names)
